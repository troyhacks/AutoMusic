{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb74832-12aa-4e35-9c84-7e3d9d415579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pathlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import psutil\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "import time\n",
    "\n",
    "# Generate a simple seed with added entropy from the current time\n",
    "seed = int(time.time() * 1000) % (2**32)\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a043b7-f16c-4cfe-b111-ca5bce58b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data_dir = 'Z:/AutoMusic/output_new_1/'\n",
    "cache_dir = 'Y:/AutoMusic/cache_4/'\n",
    "high_up_files = [os.path.join(data_dir+\"high_up_1/\", f) for f in os.listdir(data_dir+\"high_up_1/\") if os.path.isfile(os.path.join(data_dir+\"high_up_1/\", f))]\n",
    "up_test_file = random.sample(high_up_files, 1)\n",
    "test_file = up_test_file[0]\n",
    "# test_file = data_dir+\"/high_up/high_up.01_adri_block_and_paul_parsons_-_fall_in_lov_020382_027349.wav\"\n",
    "pioneer_files = 'd:/pioneer/usbanlz'\n",
    "pioneer_prefix = 'D:'\n",
    "temp_wav_file = 'Z:/AutoMusic/temp/temp.wav'\n",
    "mytotalfiles = 934 # I know this from previous runs.\n",
    "EPOCHS = 200 # using early stopping, but just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112333f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'Z:/AutoMusic/output_new_1/high_chorus_1' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_chorus_2' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_down' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_intro_1' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_intro_2' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_outro_1' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_outro_2' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_up_1' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_up_2' is clean (no changes required).\n",
      "The directory 'Z:/AutoMusic/output_new_1/high_up_3' is clean (no changes required).\n"
     ]
    }
   ],
   "source": [
    "# verify and clean the training files\n",
    "# in case the last run died or there was a mixup of the wrong files in label directories \n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def validate_and_manage_files(base_path, size_threshold, dry_run=True):\n",
    "    \"\"\"\n",
    "    Validates and manages files in immediate subdirectories:\n",
    "    - Moves files smaller than `size_threshold` bytes to the directory above `base_path` (if `dry_run` is False).\n",
    "    - Moves files to a matching directory if possible (if `dry_run` is False).\n",
    "    - Moves files to the directory above `base_path` if no matching directory exists (if `dry_run` is False).\n",
    "    - Outputs the file size for files below threshold during both dry run and execution.\n",
    "    - Prints a message if a subdirectory is clean (no changes required).\n",
    "\n",
    "    :param base_path: Path to the root directory to clean\n",
    "    :param size_threshold: Minimum file size (in bytes) for a file to be kept in the correct location\n",
    "    :param dry_run: If True, simulate actions without actually moving files\n",
    "    \"\"\"\n",
    "    # Determine the directory above the root folder\n",
    "    parent_dir = os.path.abspath(os.path.join(base_path, os.pardir))\n",
    "    \n",
    "    for sub_dir in os.listdir(base_path):\n",
    "        sub_dir_path = os.path.join(base_path, sub_dir)\n",
    "\n",
    "        # Ensure it's a directory\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            changes_made = False  # Track whether any changes are made in this subdirectory\n",
    "\n",
    "            for file_name in os.listdir(sub_dir_path):\n",
    "                file_path = os.path.join(sub_dir_path, file_name)\n",
    "\n",
    "                # Ensure it's a file\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Get file size\n",
    "                    file_size = os.path.getsize(file_path)\n",
    "\n",
    "                    # Check if file size is below threshold\n",
    "                    if file_size < size_threshold:\n",
    "                        # Simulate or perform moving small file to the parent directory\n",
    "                        new_file_path = os.path.join(parent_dir, file_name)\n",
    "                        if dry_run:\n",
    "                            print(f\"[DRY RUN] Would move '{file_name}' ({file_size} bytes, below {size_threshold} bytes) to '{parent_dir}'\")\n",
    "                        else:\n",
    "                            shutil.move(file_path, new_file_path)\n",
    "                            print(f\"Moved '{file_name}' ({file_size} bytes, below {size_threshold} bytes) to '{parent_dir}'\")\n",
    "                        changes_made = True\n",
    "                        continue\n",
    "\n",
    "                    # Check if the file name starts with the parent directory name\n",
    "                    if not file_name.startswith(sub_dir):\n",
    "                        proper_dir_path = os.path.join(base_path, sub_dir)\n",
    "\n",
    "                        # Simulate or perform moving the file to the proper directory\n",
    "                        if os.path.exists(proper_dir_path):\n",
    "                            new_file_path = os.path.join(proper_dir_path, file_name)\n",
    "                            if dry_run:\n",
    "                                print(f\"[DRY RUN] Would move '{file_name}' ({file_size} bytes) to '{proper_dir_path}'\")\n",
    "                            else:\n",
    "                                shutil.move(file_path, new_file_path)\n",
    "                                print(f\"Moved '{file_name}' ({file_size} bytes) to '{proper_dir_path}'\")\n",
    "                            changes_made = True\n",
    "                        else:\n",
    "                            # Simulate or perform moving the file to the parent directory\n",
    "                            new_file_path = os.path.join(parent_dir, file_name)\n",
    "                            if dry_run:\n",
    "                                print(f\"[DRY RUN] Would move '{file_name}' ({file_size} bytes, no matching directory found) to '{parent_dir}'\")\n",
    "                            else:\n",
    "                                shutil.move(file_path, new_file_path)\n",
    "                                print(f\"Moved '{file_name}' ({file_size} bytes, no matching directory found) to '{parent_dir}'\")\n",
    "                            changes_made = True\n",
    "\n",
    "            # If no changes were made, print a \"clean directory\" message\n",
    "            if not changes_made:\n",
    "                print(f\"The directory '{sub_dir_path}' is clean (no changes required).\")\n",
    "\n",
    "validate_and_manage_files(data_dir, size_threshold=480*1000, dry_run=False)  # Do actions, no test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7c76e0-ca62-452d-8e33-87c69329288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using run history file: ./Histories/history155.txt\n",
      "Using model save/checkpoint file: Z:/AutoMusic/checkpoint/automusic155.h5\n",
      "Sample size of 76GB should fit in free memory of 96GB - using RAM to cache\n"
     ]
    }
   ],
   "source": [
    "# Find the next history file number\n",
    "\n",
    "historycounter = 1 # start looking at this number as history1.txt\n",
    "\n",
    "Path(\"./Histories\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "while os.path.isfile(\"./Histories/history\"+str(historycounter)+\".txt\"):\n",
    "    historycounter += 1\n",
    "\n",
    "history_file = \"./Histories/history\"+str(historycounter)+\".txt\"\n",
    "print(\"Using run history file: \"+history_file)\n",
    "\n",
    "checkpoint_filepath = 'Z:/AutoMusic/checkpoint/automusic'+str(historycounter)+'.h5'\n",
    "print(\"Using model save/checkpoint file: \"+checkpoint_filepath)\n",
    "\n",
    "# Roughly figure out if the dataset will fit into memory, or we need to use disk caching\n",
    "\n",
    "free_mem = psutil.virtual_memory()\n",
    "free_mem = math.floor(free_mem.available/1024/1024/1024*0.90)\n",
    "\n",
    "samples_size = sum(f.stat().st_size for f in Path(data_dir).glob('**/*') if f.is_file())\n",
    "samples_size = math.ceil((samples_size/1024/1024/1024)*1.05)\n",
    "\n",
    "if samples_size < free_mem:\n",
    "    print(\"Sample size of \"+str(samples_size)+\"GB should fit in free memory of \"+str(free_mem)+\"GB - using RAM to cache\")\n",
    "    cache_dir = ''\n",
    "else:\n",
    "    print(\"Sample size of \"+str(samples_size)+\"GB will not fit in free memory of \"+str(int(free_mem))+\"GB - using cache dir \"+cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00469c98-ca2c-477e-85ba-474d94216d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories/Labels: ['high_chorus_1' 'high_chorus_2' 'high_down' 'high_intro_1' 'high_intro_2'\n",
      " 'high_outro_1' 'high_outro_2' 'high_up_1' 'high_up_2' 'high_up_3']\n"
     ]
    }
   ],
   "source": [
    "# List all items in data_dir\n",
    "commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "\n",
    "# Filter to include only directories and those that start with \"high_\"\n",
    "commands = np.array([cmd for cmd in commands if tf.io.gfile.isdir(f\"{data_dir}/{cmd}\") and cmd.startswith(\"high_\")])\n",
    "\n",
    "print('Directories/Labels:', commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0224dfcb-b4c8-4518-8d02-c3e8aa458ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 143535 files belonging to 10 classes.\n",
      "Using 114828 files for training.\n",
      "Using 28707 files for validation.\n",
      "\n",
      "label names: ['high_chorus_1' 'high_chorus_2' 'high_down' 'high_intro_1' 'high_intro_2'\n",
      " 'high_outro_1' 'high_outro_2' 'high_up_1' 'high_up_2' 'high_up_3']\n"
     ]
    }
   ],
   "source": [
    "# Use the generated seed in your audio dataset creation\n",
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=data_dir,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    seed=seed,  # Use the entropy-enhanced seed\n",
    "    output_sequence_length=44100*3,\n",
    "    subset='both'\n",
    ")\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cc5bfb-2936-4c1d-a7b7-fa74740943c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "  # audio = tf.squeeze(audio, axis=-1)\n",
    "  audio = audio[:,:,-1]\n",
    "  return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded6addc-4093-42a8-952a-288d2efbb695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69c9591-66b5-431a-b5cf-132ec605fe25",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 132300)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for example_audio, example_labels in train_ds.take(1):  \n",
    "  print(example_audio.shape)\n",
    "  print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c0ff93e-17b3-4a6b-9129-63463d9e3589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogMelSpectrogram(tf.keras.layers.Layer):\n",
    "    \"\"\"Compute log-magnitude mel-scaled spectrograms.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate, fft_size, hop_size, n_mels,\n",
    "                 f_min=0.0, f_max=None, **kwargs):\n",
    "        super(LogMelSpectrogram, self).__init__(**kwargs)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.fft_size = fft_size\n",
    "        self.hop_size = hop_size\n",
    "        self.n_mels = n_mels\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max if f_max else sample_rate / 2\n",
    "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=self.n_mels,\n",
    "            num_spectrogram_bins=fft_size // 2 + 1,\n",
    "            sample_rate=self.sample_rate,\n",
    "            lower_edge_hertz=self.f_min,\n",
    "            upper_edge_hertz=self.f_max)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.mel_filterbank)\n",
    "        super(LogMelSpectrogram, self).build(input_shape)\n",
    "\n",
    "    def call(self, waveforms):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : tf.Tensor, shape = (None, n_samples)\n",
    "            A Batch of mono waveforms.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)\n",
    "            The corresponding batch of log-mel-spectrograms\n",
    "        \"\"\"\n",
    "        def _tf_log10(x):\n",
    "            numerator = tf.math.log(x)\n",
    "            denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "            return numerator / denominator\n",
    "\n",
    "        def power_to_db(magnitude, amin=1e-16, top_db=80.0):\n",
    "            \"\"\"\n",
    "            https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html\n",
    "            I think this is a function in the TF supports now?\n",
    "            \"\"\"\n",
    "            ref_value = tf.reduce_max(magnitude)\n",
    "            log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))\n",
    "            log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))\n",
    "            log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
    "\n",
    "            return log_spec\n",
    "\n",
    "        spectrograms = tf.signal.stft(waveforms,\n",
    "                                      frame_length=self.fft_size,\n",
    "                                      frame_step=self.hop_size,\n",
    "                                      pad_end=False)\n",
    "\n",
    "        magnitude_spectrograms = tf.abs(spectrograms)\n",
    "\n",
    "        mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),\n",
    "                                     self.mel_filterbank)\n",
    "\n",
    "        log_mel_spectrograms = power_to_db(mel_spectrograms)\n",
    "\n",
    "        # add channel dimension\n",
    "        log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\n",
    "\n",
    "        return log_mel_spectrograms\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'fft_size': self.fft_size,\n",
    "            'hop_size': self.hop_size,\n",
    "            'n_mels': self.n_mels,\n",
    "            'sample_rate': self.sample_rate,\n",
    "            'f_min': self.f_min,\n",
    "            'f_max': self.f_max,\n",
    "        }\n",
    "        config.update(super(LogMelSpectrogram, self).get_config())\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808729a7-1356-4afe-b9f7-72f529a2ae1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 132300)]          0         \n",
      "                                                                 \n",
      " log_mel_spectrogram (LogMel  (None, 2052, 80, 1)      0         \n",
      " Spectrogram)                                                    \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 160, 80, 1)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 160, 80, 1)       320       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " gaussian_noise (GaussianNoi  (None, 160, 80, 1)       0         \n",
      " se)                                                             \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 158, 78, 32)       320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 156, 76, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 78, 38, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " spatial_dropout2d (SpatialD  (None, 78, 38, 64)       0         \n",
      " ropout2D)                                                       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 76, 36, 128)       73856     \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " gaussian_dropout (GaussianD  (None, 128)              0         \n",
      " ropout)                                                         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,794\n",
      "Trainable params: 110,634\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = example_audio.shape[1:]\n",
    "input_length = len(input_shape)\n",
    "num_labels = len(label_names)\n",
    "\n",
    "frame_length=1024\n",
    "frame_step=64 # 128\n",
    "fft_length=None\n",
    "sample_rate=44100\n",
    "duration=3\n",
    "num_mel_channels=80\n",
    "freq_min=40 # 200\n",
    "freq_max=10000 # 8000\n",
    "\n",
    "def ConvModel(n_classes=num_labels, sample_rate=sample_rate, duration=duration,\n",
    "              fft_size=frame_length, hop_size=frame_step, n_mels=num_mel_channels):\n",
    "    \n",
    "    n_samples = sample_rate * duration\n",
    "    \n",
    "    # Accept raw audio data as input\n",
    "    x = layers.Input(shape=(n_samples,), name='input', dtype='float32')\n",
    "    y = LogMelSpectrogram(sample_rate, fft_size, hop_size, n_mels, freq_min, freq_max)(x)\n",
    "    y = layers.Resizing(160,80)(y)\n",
    "    y = layers.BatchNormalization(axis=2)(y)\n",
    "    y = layers.GaussianNoise(0.5)(y)\n",
    "    y = layers.Conv2D(32, 3, activation='relu')(y)\n",
    "    y = layers.Conv2D(64, 3, activation='relu')(y)\n",
    "    y = layers.MaxPooling2D()(y)\n",
    "    y = layers.SpatialDropout2D(0.25)(y)\n",
    "    y = layers.Conv2D(128, 3, activation='relu')(y)  # Additional Conv2D layer\n",
    "    y = layers.GlobalAveragePooling2D()(y)  \n",
    "    y = layers.Flatten()(y)\n",
    "    y = layers.Dense(128, activation='relu')(y)\n",
    "    y = layers.GaussianDropout(0.5)(y)\n",
    "    y = layers.Dense(num_labels)(y)\n",
    "    \n",
    "    return tf.keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "model = ConvModel()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# # Define the learning rate schedule\n",
    "# initial_learning_rate = 0.01\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=100000,\n",
    "#     decay_rate=0.96,\n",
    "#     staircase=True\n",
    "# )\n",
    "\n",
    "# # Compile the model with the optimizer using the learning rate schedule\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy'],\n",
    "# )\n",
    "             \n",
    "# learning_rate = 0.001 # 0.0001\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb42e65a-0658-4f63-87d5-cd4f6145600b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cache_dir != '':\n",
    "    for f in Path(cache_dir).glob('*'):\n",
    "        try:\n",
    "            print(\"Removing cache file \"+str(f))\n",
    "            f.unlink()\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (f, e.strerror))\n",
    "    \n",
    "train_ds = train_ds.cache(cache_dir).shuffle(buffer_size=1000, reshuffle_each_iteration=True).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache(cache_dir).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_precache = test_ds\n",
    "test_ds = test_ds.cache(cache_dir).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761b0a0e-b911-4b67-8dcf-154a1f73a7b5",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previous checkpoint 154\n",
      "Epoch 1/200\n",
      "3588/3589 [============================>.] - ETA: 0s - loss: 1.0609 - accuracy: 0.6081\n",
      "Epoch 1: val_accuracy improved from -inf to 0.75340, saving model to Z:/AutoMusic/checkpoint\\automusic155.h5\n",
      "3589/3589 [==============================] - 121s 28ms/step - loss: 1.0609 - accuracy: 0.6081 - val_loss: 0.7228 - val_accuracy: 0.7534\n",
      "Epoch 2/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0533 - accuracy: 0.6088\n",
      "Epoch 2: val_accuracy did not improve from 0.75340\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0533 - accuracy: 0.6088 - val_loss: 0.7244 - val_accuracy: 0.7534\n",
      "Epoch 3/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0484 - accuracy: 0.6119\n",
      "Epoch 3: val_accuracy did not improve from 0.75340\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0484 - accuracy: 0.6118 - val_loss: 0.7321 - val_accuracy: 0.7512\n",
      "Epoch 4/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0446 - accuracy: 0.6121\n",
      "Epoch 4: val_accuracy did not improve from 0.75340\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0446 - accuracy: 0.6121 - val_loss: 0.7368 - val_accuracy: 0.7507\n",
      "Epoch 5/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0402 - accuracy: 0.6120\n",
      "Epoch 5: val_accuracy did not improve from 0.75340\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 1.0402 - accuracy: 0.6120 - val_loss: 0.7414 - val_accuracy: 0.7518\n",
      "Epoch 6/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0365 - accuracy: 0.6173\n",
      "Epoch 6: val_accuracy improved from 0.75340 to 0.75354, saving model to Z:/AutoMusic/checkpoint\\automusic155.h5\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0365 - accuracy: 0.6173 - val_loss: 0.7199 - val_accuracy: 0.7535\n",
      "Epoch 7/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0330 - accuracy: 0.6173\n",
      "Epoch 7: val_accuracy did not improve from 0.75354\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0330 - accuracy: 0.6173 - val_loss: 0.7477 - val_accuracy: 0.7434\n",
      "Epoch 8/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0319 - accuracy: 0.6167\n",
      "Epoch 8: val_accuracy did not improve from 0.75354\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0319 - accuracy: 0.6167 - val_loss: 0.7262 - val_accuracy: 0.7475\n",
      "Epoch 9/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0255 - accuracy: 0.6193\n",
      "Epoch 9: val_accuracy did not improve from 0.75354\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 1.0256 - accuracy: 0.6192 - val_loss: 0.7370 - val_accuracy: 0.7457\n",
      "Epoch 10/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0273 - accuracy: 0.6174\n",
      "Epoch 10: val_accuracy did not improve from 0.75354\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0273 - accuracy: 0.6174 - val_loss: 0.7273 - val_accuracy: 0.7498\n",
      "Epoch 11/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.6194\n",
      "Epoch 11: val_accuracy improved from 0.75354 to 0.75619, saving model to Z:/AutoMusic/checkpoint\\automusic155.h5\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0260 - accuracy: 0.6194 - val_loss: 0.7150 - val_accuracy: 0.7562\n",
      "Epoch 12/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0224 - accuracy: 0.6196\n",
      "Epoch 12: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0224 - accuracy: 0.6196 - val_loss: 0.7217 - val_accuracy: 0.7547\n",
      "Epoch 13/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0207 - accuracy: 0.6217\n",
      "Epoch 13: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0207 - accuracy: 0.6217 - val_loss: 0.7277 - val_accuracy: 0.7502\n",
      "Epoch 14/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0198 - accuracy: 0.6219\n",
      "Epoch 14: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0198 - accuracy: 0.6219 - val_loss: 0.7244 - val_accuracy: 0.7528\n",
      "Epoch 15/200\n",
      "3588/3589 [============================>.] - ETA: 0s - loss: 1.0207 - accuracy: 0.6192\n",
      "Epoch 15: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0207 - accuracy: 0.6192 - val_loss: 0.7298 - val_accuracy: 0.7508\n",
      "Epoch 16/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0152 - accuracy: 0.6222\n",
      "Epoch 16: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 83s 23ms/step - loss: 1.0152 - accuracy: 0.6222 - val_loss: 0.7332 - val_accuracy: 0.7463\n",
      "Epoch 17/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0154 - accuracy: 0.6232\n",
      "Epoch 17: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0154 - accuracy: 0.6232 - val_loss: 0.7261 - val_accuracy: 0.7496\n",
      "Epoch 18/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0099 - accuracy: 0.6244\n",
      "Epoch 18: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0099 - accuracy: 0.6244 - val_loss: 0.7309 - val_accuracy: 0.7465\n",
      "Epoch 19/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0105 - accuracy: 0.6245\n",
      "Epoch 19: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0105 - accuracy: 0.6245 - val_loss: 0.7220 - val_accuracy: 0.7511\n",
      "Epoch 20/200\n",
      "3588/3589 [============================>.] - ETA: 0s - loss: 1.0134 - accuracy: 0.6235\n",
      "Epoch 20: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 1.0134 - accuracy: 0.6235 - val_loss: 0.7359 - val_accuracy: 0.7537\n",
      "Epoch 21/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.6255\n",
      "Epoch 21: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0068 - accuracy: 0.6255 - val_loss: 0.7458 - val_accuracy: 0.7438\n",
      "Epoch 22/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0088 - accuracy: 0.6252\n",
      "Epoch 22: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0088 - accuracy: 0.6252 - val_loss: 0.7466 - val_accuracy: 0.7383\n",
      "Epoch 23/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0083 - accuracy: 0.6266\n",
      "Epoch 23: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 1.0082 - accuracy: 0.6266 - val_loss: 0.7279 - val_accuracy: 0.7494\n",
      "Epoch 24/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0043 - accuracy: 0.6267\n",
      "Epoch 24: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 1.0043 - accuracy: 0.6267 - val_loss: 0.7468 - val_accuracy: 0.7459\n",
      "Epoch 25/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0038 - accuracy: 0.6255\n",
      "Epoch 25: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 1.0039 - accuracy: 0.6255 - val_loss: 0.7129 - val_accuracy: 0.7491\n",
      "Epoch 26/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0038 - accuracy: 0.6287\n",
      "Epoch 26: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 1.0038 - accuracy: 0.6287 - val_loss: 0.7108 - val_accuracy: 0.7537\n",
      "Epoch 27/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 1.0013 - accuracy: 0.6274\n",
      "Epoch 27: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 1.0013 - accuracy: 0.6274 - val_loss: 0.7181 - val_accuracy: 0.7524\n",
      "Epoch 28/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 1.0004 - accuracy: 0.6273\n",
      "Epoch 28: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 1.0004 - accuracy: 0.6273 - val_loss: 0.7147 - val_accuracy: 0.7492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9982 - accuracy: 0.6297\n",
      "Epoch 29: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9982 - accuracy: 0.6297 - val_loss: 0.7253 - val_accuracy: 0.7462\n",
      "Epoch 30/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9985 - accuracy: 0.6296\n",
      "Epoch 30: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 0.9985 - accuracy: 0.6296 - val_loss: 0.7121 - val_accuracy: 0.7518\n",
      "Epoch 31/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9983 - accuracy: 0.6298\n",
      "Epoch 31: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9983 - accuracy: 0.6299 - val_loss: 0.7232 - val_accuracy: 0.7496\n",
      "Epoch 32/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9952 - accuracy: 0.6313\n",
      "Epoch 32: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9953 - accuracy: 0.6313 - val_loss: 0.7250 - val_accuracy: 0.7517\n",
      "Epoch 33/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9977 - accuracy: 0.6306\n",
      "Epoch 33: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 0.9977 - accuracy: 0.6306 - val_loss: 0.7253 - val_accuracy: 0.7482\n",
      "Epoch 34/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9924 - accuracy: 0.6305\n",
      "Epoch 34: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9924 - accuracy: 0.6305 - val_loss: 0.7108 - val_accuracy: 0.7537\n",
      "Epoch 35/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9894 - accuracy: 0.6333\n",
      "Epoch 35: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9893 - accuracy: 0.6333 - val_loss: 0.7121 - val_accuracy: 0.7507\n",
      "Epoch 36/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9909 - accuracy: 0.6319\n",
      "Epoch 36: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 0.9909 - accuracy: 0.6319 - val_loss: 0.7199 - val_accuracy: 0.7519\n",
      "Epoch 37/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.6302\n",
      "Epoch 37: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9954 - accuracy: 0.6302 - val_loss: 0.7277 - val_accuracy: 0.7541\n",
      "Epoch 38/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9914 - accuracy: 0.6311\n",
      "Epoch 38: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 0.9914 - accuracy: 0.6310 - val_loss: 0.7193 - val_accuracy: 0.7459\n",
      "Epoch 39/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9890 - accuracy: 0.6333\n",
      "Epoch 39: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 22ms/step - loss: 0.9890 - accuracy: 0.6333 - val_loss: 0.7216 - val_accuracy: 0.7505\n",
      "Epoch 40/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9862 - accuracy: 0.6341\n",
      "Epoch 40: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9862 - accuracy: 0.6341 - val_loss: 0.7191 - val_accuracy: 0.7478\n",
      "Epoch 41/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9893 - accuracy: 0.6327\n",
      "Epoch 41: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9893 - accuracy: 0.6327 - val_loss: 0.7294 - val_accuracy: 0.7455\n",
      "Epoch 42/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9844 - accuracy: 0.6356\n",
      "Epoch 42: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9845 - accuracy: 0.6356 - val_loss: 0.7283 - val_accuracy: 0.7433\n",
      "Epoch 43/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9846 - accuracy: 0.6348\n",
      "Epoch 43: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9846 - accuracy: 0.6348 - val_loss: 0.7197 - val_accuracy: 0.7468\n",
      "Epoch 44/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9866 - accuracy: 0.6331\n",
      "Epoch 44: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9866 - accuracy: 0.6331 - val_loss: 0.7244 - val_accuracy: 0.7445\n",
      "Epoch 45/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9832 - accuracy: 0.6339\n",
      "Epoch 45: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9831 - accuracy: 0.6340 - val_loss: 0.7176 - val_accuracy: 0.7480\n",
      "Epoch 46/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9812 - accuracy: 0.6342\n",
      "Epoch 46: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9812 - accuracy: 0.6342 - val_loss: 0.7199 - val_accuracy: 0.7452\n",
      "Epoch 47/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9836 - accuracy: 0.6344\n",
      "Epoch 47: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9837 - accuracy: 0.6344 - val_loss: 0.7102 - val_accuracy: 0.7504\n",
      "Epoch 48/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9838 - accuracy: 0.6337\n",
      "Epoch 48: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9838 - accuracy: 0.6337 - val_loss: 0.7076 - val_accuracy: 0.7544\n",
      "Epoch 49/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9789 - accuracy: 0.6364\n",
      "Epoch 49: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 79s 22ms/step - loss: 0.9789 - accuracy: 0.6364 - val_loss: 0.7097 - val_accuracy: 0.7543\n",
      "Epoch 50/200\n",
      "3588/3589 [============================>.] - ETA: 0s - loss: 0.9813 - accuracy: 0.6361\n",
      "Epoch 50: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 80s 22ms/step - loss: 0.9813 - accuracy: 0.6361 - val_loss: 0.7155 - val_accuracy: 0.7513\n",
      "Epoch 51/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9784 - accuracy: 0.6365\n",
      "Epoch 51: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 0.9784 - accuracy: 0.6365 - val_loss: 0.7063 - val_accuracy: 0.7530\n",
      "Epoch 52/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9757 - accuracy: 0.6368\n",
      "Epoch 52: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 81s 23ms/step - loss: 0.9757 - accuracy: 0.6368 - val_loss: 0.7076 - val_accuracy: 0.7532\n",
      "Epoch 53/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9786 - accuracy: 0.6376\n",
      "Epoch 53: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 0.9786 - accuracy: 0.6376 - val_loss: 0.7246 - val_accuracy: 0.7469\n",
      "Epoch 54/200\n",
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9774 - accuracy: 0.6374\n",
      "Epoch 54: val_accuracy did not improve from 0.75619\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 0.9774 - accuracy: 0.6374 - val_loss: 0.7143 - val_accuracy: 0.7541\n",
      "Epoch 55/200\n",
      "3587/3589 [============================>.] - ETA: 0s - loss: 0.9772 - accuracy: 0.6388\n",
      "Epoch 55: val_accuracy improved from 0.75619 to 0.75877, saving model to Z:/AutoMusic/checkpoint\\automusic155.h5\n",
      "3589/3589 [==============================] - 82s 23ms/step - loss: 0.9773 - accuracy: 0.6388 - val_loss: 0.6973 - val_accuracy: 0.7588\n",
      "Epoch 56/200\n",
      "3588/3589 [============================>.] - ETA: 0s - loss: 0.9740 - accuracy: 0.6381\n",
      "Epoch 56: val_accuracy did not improve from 0.75877\n",
      "3589/3589 [==============================] - 138s 38ms/step - loss: 0.9739 - accuracy: 0.6381 - val_loss: 0.7087 - val_accuracy: 0.7567\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589/3589 [==============================] - ETA: 0s - loss: 0.9745 - accuracy: 0.6399"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ:/AutoMusic/checkpoint/automusic\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistorycounter\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# our manually picked \"best\" scoring model so far.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing previous checkpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistorycounter\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearlystop_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ:/AutoMusic/checkpoint/automusic/automusic_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistorycounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1)\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    verbose=1,\n",
    "    patience=25,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Define the learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model with the optimizer using the learning rate schedule\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# history = model_optimized.fit(train_ds, training_labels, epochs=50)\n",
    "\n",
    "# This may be a bad idea:\n",
    "# we're still retraining on ALL data, but this seems to speed up the epochs needed:\n",
    "# model.load_weights(checkpoint_filepath) # kick off training with the last run's weights\n",
    "\n",
    "if (os.path.isfile(f\"Z:/AutoMusic/checkpoint/automusic{historycounter}.h5\")):\n",
    "    model.load_weights(f\"Z:/AutoMusic/checkpoint/automusic{historycounter}.h5\") \n",
    "    print(f\"Resuming current checkpoint {historycounter} to resume.\") \n",
    "elif (os.path.isfile(f\"Z:/AutoMusic/checkpoint/automusic{historycounter-1}.h5\")):\n",
    "    model.load_weights(f\"Z:/AutoMusic/checkpoint/automusic{historycounter-1}.h5\")\n",
    "    print(f\"Loading previous checkpoint {historycounter-1} weights.\")\n",
    "    \n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[earlystop_callback,checkpoint_callback]\n",
    ")\n",
    "\n",
    "model.save(f\"Z:/AutoMusic/checkpoint/automusic/automusic_model_{historycounter}.h5\")\n",
    "\n",
    "# shouldn't be needed with restore_best_weights=True\n",
    "# model.load_weights(checkpoint_filepath) # load the best saved weights... even if they aren't from this run, maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16258a-fd90-4e98-a181-cdb8078b107c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if 'history' is defined\n",
    "    if history:\n",
    "        metrics = history.history\n",
    "        plt.figure(figsize=(16,6))\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "        plt.legend(['loss', 'val_loss'])\n",
    "        plt.ylim([0, max(plt.ylim())])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "        plt.legend(['accuracy', 'val_accuracy'])\n",
    "        plt.ylim([0, 100])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy [%]')\n",
    "    else:\n",
    "        print(\"Training Skipped. Validation only.\")\n",
    "except NameError:\n",
    "    print(\"The 'history' variable is not defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a37776-8f25-4432-a059-39ab95680916",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = tf.concat(list(test_ds_precache.map(lambda s,lab: lab)), axis=0)\n",
    "y_pred = model.predict(test_ds_precache)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.title(f'AutoMusic - Electronic Music Phrase Classifier (Run {historycounter})')\n",
    "plt.savefig(f'Z:/AutoMusic/AutoMusic_HeatMap_{historycounter}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebcff3-b636-4474-a160-718a0e72a4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = test_file\n",
    "x = tf.io.read_file(str(x))\n",
    "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=44100*3)\n",
    "x = tf.squeeze(x, axis=-1)\n",
    "x = x[tf.newaxis,...]\n",
    "print(x.shape)\n",
    "\n",
    "prediction = model(x)\n",
    "plt.bar(label_names, tf.nn.softmax(prediction[0]))\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def delete_random_files(directory, keep_count):\n",
    "    # List all files in the directory\n",
    "    all_files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    # Select X random files to keep\n",
    "    keep_files = random.sample(all_files, keep_count)\n",
    "    \n",
    "    # Delete the rest of the files\n",
    "    for file in all_files:\n",
    "        if file not in keep_files:\n",
    "            os.remove(file)\n",
    "            # print(f\"Deleted: {file}\")\n",
    "    \n",
    "    print(\"Operation completed successfully.\")\n",
    "\n",
    "# number is number of files to KEEP\n",
    "# delete_random_files(\"Z:/AutoMusic/output/high_chorus/\", 2500)\n",
    "# delete_random_files(\"Z:/AutoMusic/output/high_down/\", 2500)\n",
    "# delete_random_files(\"Z:/AutoMusic/output/high_intro/\", 2500)\n",
    "# delete_random_files(\"Z:/AutoMusic/output/high_outro/\", 2500)\n",
    "# delete_random_files(\"Z:/AutoMusic/output/high_up/\", 2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9706c-7cd1-459c-81f9-fa5aa53d718e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyrekordbox\n",
    "from pyrekordbox.anlz import AnlzFile\n",
    "from pydub import AudioSegment\n",
    "from pydub.generators import WhiteNoise\n",
    "from pydub.effects import speedup, normalize\n",
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import logging\n",
    "l = logging.getLogger(\"pydub.converter\")\n",
    "l.setLevel(logging.CRITICAL)\n",
    "\n",
    "myfiles = pyrekordbox.anlz.walk_anlz_paths(pioneer_files)\n",
    "\n",
    "myfilecounter = 0\n",
    "\n",
    "with open(history_file,'w') as out:\n",
    "\n",
    "    for myfoundfile in myfiles:\n",
    "\n",
    "        try: mydat = AnlzFile.parse_file(myfoundfile[1]['DAT'])\n",
    "        except: continue\n",
    "        try: myext = AnlzFile.parse_file(myfoundfile[1]['EXT'])\n",
    "        except: continue\n",
    "\n",
    "        mymp3 = mydat.get('PPTH')\n",
    "        mymp3 = pioneer_prefix + mymp3\n",
    "\n",
    "        if \"Ultimate\" in str(mymp3):\n",
    "            # this is very specific to my library where there's some weird files full of random loops\n",
    "            continue\n",
    "\n",
    "        # print(mymp3)\n",
    "\n",
    "        mylabels = {}\n",
    "        mylabels['high'] = ['unknown', 'high_intro_', 'high_up_', 'high_down', 'unknown', 'high_chorus_', 'high_outro_', 'unknown', 'unknown', 'unknown', 'unknown']\n",
    "        mylabels['mid']  = ['unknown', 'mid_intro', 'mid_verse_1', 'mid_verse_2', 'mid_verse_3', 'mid_verse_4', 'mid_verse_5', 'mid_verse_5', 'mid_bridge', 'mid_chorus', 'mid_outro']\n",
    "        mylabels['low']  = ['unknown', 'low_intro', 'low_verse_1', 'low_verse_1', 'low_verse_1', 'low_verse_2', 'low_verse_2', 'low_verse_2', 'low_bridge', 'low_chorus', 'low_outro']\n",
    "\n",
    "        try: mytimecode = mydat.get('PQTZ')[2]\n",
    "        except: continue\n",
    "        try: mystructures = myext.get('PSSI').entries\n",
    "        except: continue\n",
    "        try: mymood = myext.get('PSSI').mood\n",
    "        except: continue\n",
    "\n",
    "        if (mymood == 1):\n",
    "            mymood = \"high\"\n",
    "        elif (mymood == 2):\n",
    "            mymood = \"mid\"\n",
    "            continue\n",
    "        elif (mymood == 3):\n",
    "            mymood = \"low\"\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        mytimecode = mydat.get('PQTZ')[2]\n",
    "\n",
    "        myfilelastbeat = myext.get('PSSI').end_beat\n",
    "\n",
    "        mysimplestructure = {}\n",
    "\n",
    "        myfilecounter += 1\n",
    "\n",
    "        for x in range(len(mystructures)):\n",
    "\n",
    "            mylabel = mystructures[x].kind\n",
    "\n",
    "            if (mymood == \"high\"):\n",
    "\n",
    "                if (mylabel == 1 or mylabel == 5 or mylabel == 6):\n",
    "                    if (mystructures[x].k1 == 1):\n",
    "                        mylabelmodifier = \"1\"\n",
    "                    else:\n",
    "                        mylabelmodifier = \"2\"\n",
    "                elif (mylabel == 2):\n",
    "                    if (mystructures[x].k2 == 0 and mystructures[x].k3 == 0):\n",
    "                        mylabelmodifier = \"1\"\n",
    "                    elif (mystructures[x].k2 == 0 and mystructures[x].k3 == 1):\n",
    "                        mylabelmodifier = \"2\"\n",
    "                    elif (mystructures[x].k2 == 1 and mystructures[x].k3 == 0):\n",
    "                        mylabelmodifier = \"3\"\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    mylabelmodifier = \"\"\n",
    "\n",
    "            else:\n",
    "                mylabelmodifier = \"\"\n",
    "            \n",
    "            mylabel = mylabels[mymood][mylabel]+mylabelmodifier\n",
    "\n",
    "            myfirstbeat = mystructures[x].beat\n",
    "\n",
    "            if (x+1<len(mystructures)):\n",
    "                mylastbeat = mystructures[x+1].beat\n",
    "            else:\n",
    "                mylastbeat = myfilelastbeat\n",
    "\n",
    "            if (mylastbeat+1<len(mytimecode)):\n",
    "                mylastbeat = mylastbeat\n",
    "            else:\n",
    "                mylastbeat = len(mytimecode)-1\n",
    "\n",
    "            mysegmentbars = (mylastbeat-myfirstbeat)//8\n",
    "            mysegmentleftovers = (mylastbeat-myfirstbeat)%8\n",
    "\n",
    "            myfirsttimecode = mytimecode[int(myfirstbeat)]*1000\n",
    "            mylasttimecode = mytimecode[int(mylastbeat)]*1000\n",
    "\n",
    "            mysimplestructure[int(myfirsttimecode)] = mylabel\n",
    "            mysimplestructure[int(mylasttimecode-1)] = mylabel\n",
    "\n",
    "        mysong = AudioSegment.from_file(mymp3,frame_rate=44100)\n",
    "        mono_audios = mysong.split_to_mono() \n",
    "        mysongmono = mono_audios[0]\n",
    "        # chomp output with a random start time of 0..2999ms into the file, to give some variations.\n",
    "        myrandomoffset = random.randint(0,2999)\n",
    "        # mysongmono = mysongmono.normalize()\n",
    "\n",
    "        mysongmono[myrandomoffset:].export(temp_wav_file,format=\"wav\")\n",
    "\n",
    "        x = tf.io.read_file(temp_wav_file)\n",
    "        x, sample_rate = tf.audio.decode_wav(x, desired_channels=1)\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        waveform = x\n",
    "\n",
    "        slices = int(waveform.shape[0] / (44100*3))\n",
    "        samples = tf.split(waveform[: slices * (44100*3)], slices)\n",
    "\n",
    "        milliseconds = 0\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        transitions = 0\n",
    "\n",
    "        mylabelsseen = []\n",
    "        fileview = \"# \"\n",
    "\n",
    "        currentwrongs = 0\n",
    "\n",
    "        for sample in samples:\n",
    "\n",
    "            x = sample[tf.newaxis,...]\n",
    "\n",
    "            prediction = model(x)\n",
    "\n",
    "            # plt.bar(label_names, tf.nn.softmax(prediction[0]))\n",
    "            # plt.show()\n",
    "\n",
    "            # correct label for random offset above, because file may be skewed 1..2999ms ahead\n",
    "            #\n",
    "            res = mysimplestructure.get(milliseconds) or mysimplestructure[\n",
    "                  min(mysimplestructure.keys(), key = lambda key: abs(key-milliseconds-myrandomoffset))]\n",
    "\n",
    "            res2 = mysimplestructure.get(milliseconds) or mysimplestructure[\n",
    "                   min(mysimplestructure.keys(), key = lambda key: abs(key-milliseconds-myrandomoffset+3000))]\n",
    "\n",
    "            myaactualendlabel = str(res2)\n",
    "\n",
    "            mypredictedlabel = str(label_names[np.argmax(prediction)])\n",
    "            myactuallabel = str(res)\n",
    "\n",
    "            myrandom = random.randint(0,9)\n",
    "            \n",
    "            if myactuallabel != myaactualendlabel :\n",
    "\n",
    "                # print(\" ~~~ \"+mypredictedlabel+\" in transition from \"+myactuallabel+\" to \"+myaactualendlabel\n",
    "                print(\"~\", end=\"\")\n",
    "                print(\"~\", end=\"\",file=out)\n",
    "                transitions += 1\n",
    "                currentwrongs = 0\n",
    "\n",
    "            if mypredictedlabel == myactuallabel :\n",
    "\n",
    "                right += 1\n",
    "                currentwrongs = 0\n",
    "                mylabeldir = myactuallabel\n",
    "\n",
    "                if myrandom == 0:\n",
    "                    print(\"C\", end=\"\")\n",
    "                    print(\"C\", end=\"\",file=out)\n",
    "                    myoutputbasefile = myactuallabel+\".\"+Path(mymp3).stem+\"_\"+str(int(milliseconds)-myrandomoffset).rjust(6,'0')+\"_\"+str(int(milliseconds+3000)-myrandomoffset).rjust(6,'0')\n",
    "                    mytempsong = mysong[milliseconds:milliseconds+3000]\n",
    "                    mytempsong.export(data_dir+\"/\"+myactuallabel+\"/\"+myoutputbasefile+\"_CORRECT.wav\", format=\"wav\")\n",
    "                else:\n",
    "                    print(\"+\", end=\"\")\n",
    "                    print(\"+\", end=\"\",file=out)\n",
    "                        \n",
    "            else:\n",
    "\n",
    "                wrong += 1\n",
    "                \n",
    "                if (myactuallabel not in mylabelsseen or currentwrongs > 0 or myrandom == 0): # 10% chance of random sampling\n",
    "\n",
    "                    currentwrongs += 1\n",
    "\n",
    "                    if milliseconds+3001 < len(mysong):\n",
    "\n",
    "                        mylabelsseen.append(myactuallabel)\n",
    "                        mylabeldir = myactuallabel\n",
    "\n",
    "                        myoutputbasefile = myactuallabel+\".\"+Path(mymp3).stem+\"_\"+str(int(milliseconds)-myrandomoffset).rjust(6,'0')+\"_\"+str(int(milliseconds+3000)-myrandomoffset).rjust(6,'0')\n",
    "\n",
    "#                         myspeedup = mysong[milliseconds:milliseconds+3000].speedup(1.03)\n",
    "#                         mynoise = WhiteNoise().to_audio_segment(duration=len(myspeedup)).apply_gain(-20)\n",
    "#                         myspeedup_noise = myspeedup.overlay(mynoise)\n",
    "\n",
    "                        mytempsong = mysong[milliseconds:milliseconds+3000]\n",
    "                        mynoise = WhiteNoise().to_audio_segment(duration=len(mytempsong)).apply_gain(-20)\n",
    "                        mytempsong_noise = mytempsong.overlay(mynoise)\n",
    "\n",
    "                        mytempsong.export(data_dir+\"/\"+myactuallabel+\"/\"+myoutputbasefile+\"_FIXES.wav\", format=\"wav\")\n",
    "#                         myspeedup.export(data_dir+\"/\"+myactuallabel+\"/\"+myoutputbasefile+\"_speed_FIXES_N.wav\", format=\"wav\")\n",
    "#                         myspeedup_noise.export(data_dir+\"/\"+myactuallabel+\"/\"+myoutputbasefile+\"_speed_noise_FIXES_N.wav\", format=\"wav\")\n",
    "                        mytempsong_noise.export(data_dir+\"/\"+myactuallabel+\"/\"+myoutputbasefile+\"_noise_FIXES_N.wav\", format=\"wav\")\n",
    "\n",
    "                        if myrandom == 0:\n",
    "                            print(\"R\", end=\"\")\n",
    "                            print(\"R\", end=\"\",file=out)\n",
    "                            currentwrongs = 0 # we only want one random sample, just to spice things up.\n",
    "                            # this also functions as a \"stop writing sequence of samples early\" randomness.\n",
    "                        else:\n",
    "                            print(\"W\", end=\"\")\n",
    "                            print(\"W\", end=\"\",file=out)\n",
    "\n",
    "                        if currentwrongs > 10: # stop over-feeding a LOT of wrong samples to the model next time. Let myrandom take care of this.\n",
    "                            currentwrongs = 0\n",
    "\n",
    "                    else:\n",
    "                        print(\"!\", end=\"\")\n",
    "                        print(\"!\", end=\"\",file=out)\n",
    "                else:\n",
    "                    print(\"!\", end=\"\")\n",
    "                    print(\"!\", end=\"\",file=out)\n",
    "\n",
    "            milliseconds += 3000\n",
    "\n",
    "        print(\" \")\n",
    "        print(\" \",file=out)\n",
    "        print(str(int(right/(right+wrong)*100))+\"% correct - \"+str(Path(mymp3).name)+\" (\"+str(myfilecounter)+\" of \"+str(mytotalfiles)+\")\")\n",
    "        print(str(int(right/(right+wrong)*100))+\"% correct - \"+str(Path(mymp3).name)+\" (\"+str(myfilecounter)+\" of \"+str(mytotalfiles)+\")\",file=out)\n",
    "        print(\" \")\n",
    "        print(\" \",file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b6390-7fc1-4f83-8dfb-94b93fdc2037",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from collections import defaultdict\n",
    "\n",
    "counter = 113\n",
    "histories = []\n",
    "histories_score = defaultdict(int)\n",
    "\n",
    "print(\"found history:\")\n",
    "\n",
    "while os.path.isfile(\"./Histories/history\"+str(counter)+\".txt\"):\n",
    "    print(str(counter),end=' ')\n",
    "    histories.append(open(\"./Histories/history\"+str(counter)+\".txt\",'r').readlines())\n",
    "    counter += 1\n",
    "print()\n",
    "\n",
    "best = 0\n",
    "\n",
    "with open('./Histories/history_all.txt','w') as out:\n",
    "    \n",
    "    for z in range(len(histories[0])):\n",
    "        \n",
    "        for x in range(len(histories)):\n",
    "            if histories[x][z][0].isdigit():\n",
    "                testbest = int(histories[x][z].rstrip().partition(\"%\")[0])\n",
    "                if testbest > best:\n",
    "                    best = testbest\n",
    "                histories_score[x] += testbest\n",
    "                \n",
    "        for x in range(len(histories)):\n",
    "            if histories[x][z].rstrip() == \"\":\n",
    "                print(\"\",file=out)\n",
    "                break\n",
    "            else:\n",
    "                if histories[x][z][0].isdigit():\n",
    "                    testbest = int(histories[x][z].rstrip().partition(\"%\")[0])\n",
    "                    if best == testbest:\n",
    "                        print(\"> \",file=out,end='')\n",
    "                    else:\n",
    "                        print(\"  \",file=out,end='')\n",
    "                print(histories[x][z].rstrip(),file=out)\n",
    "                \n",
    "        best = 0\n",
    "\n",
    "print(\" --------------- \")\n",
    "\n",
    "start_counter = 113\n",
    "\n",
    "checkpoint_dir = 'Z:/AutoMusic/checkpoint/'\n",
    "\n",
    "for x in range(len(histories_score)):\n",
    "    print(\"Run \", end='')\n",
    "    print(start_counter, end='')\n",
    "    print(\" = \", end='')\n",
    "    print(round(histories_score[x] / 934, 1), end='')\n",
    "    print(\"%\")\n",
    "    start_counter += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
